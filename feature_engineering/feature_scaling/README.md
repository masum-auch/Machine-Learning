Feature Scaling

This folder contains notebooks and scripts related to feature scaling, 
which is an essential preprocessing step in machine learning to ensure that all features contribute equally to the model. 
Feature scaling improves model performance, convergence speed, and stability.  

Topics Covered

1. Standardization (Z-score Scaling):-  
   Scaling features to have a mean of 0 and a standard deviation of 1.

2. Normalization (Min-Max Scaling):-  
   Rescaling features to a fixed range, typically [0, 1].

3. Robust Scaling:-  
   Scaling features using statistics that are robust to outliers, such as the median and interquartile range (IQR).

4. Max Abs Scaling:-  
   Scaling features by their maximum absolute value, keeping sparse data intact.

5. Practical Use Cases:- 
   - Before gradient-based models (like Linear Regression, Neural Networks)  
   - Before distance-based models (like KNN, SVM)  
   - When features have different units or magnitudes  

---

This folder serves as a reference and workspace for applying feature scaling techniques to improve model performance and ensure consistent feature ranges across datasets.


Author -----
Masum(AI Engineer)
